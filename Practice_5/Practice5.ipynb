{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое занятие №5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Необходимые сегодня библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torch\n",
    "* timm\n",
    "* scikit-learn\n",
    "* pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Автоэнкодер](https://neurohive.io/ru/osnovy-data-science/avtojenkoder-tipy-arhitektur-i-primenenie/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализация конфига для дальнейшей работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size'    : 32,\n",
    "    'total_epochs'  : 40,\n",
    "    'learning_rate' : 1e-4,\n",
    "    'save_path'     : './weights', # путь куда сохранять модельки\n",
    "    'seed'          : 42,\n",
    "    'dim_code'      : 512 # длина вектора латентного пространства\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(config['save_path']):\n",
    "    os.mkdir(config['save_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = config['save_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать базы лиц [LFW](https://vis-www.cs.umass.edu/lfw/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataset(attrs_name = \"lfw_attributes.txt\",\n",
    "                      images_name = \"lfw-deepfunneled\",\n",
    "                      dx=80,dy=80,\n",
    "                      dimx=64,dimy=64\n",
    "    ):\n",
    "\n",
    "    #download if not exists\n",
    "    if not os.path.exists(images_name):\n",
    "        print(\"images not found, donwloading...\")\n",
    "        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n",
    "        print(\"extracting...\")\n",
    "        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n",
    "        print(\"done\")\n",
    "        assert os.path.exists(images_name)\n",
    "\n",
    "    if not os.path.exists(attrs_name):\n",
    "        print(\"attributes not found, downloading...\")\n",
    "        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n",
    "        print(\"done\")\n",
    "\n",
    "    #read attrs\n",
    "    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,)\n",
    "    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n",
    "\n",
    "    #read photos\n",
    "    photo_ids = []\n",
    "    for dirpath, dirnames, filenames in os.walk(images_name):\n",
    "        for fname in filenames:\n",
    "            if fname.endswith(\".jpg\"):\n",
    "                fpath = os.path.join(dirpath,fname)\n",
    "                photo_id = fname[:-4].replace('_',' ').split()\n",
    "                person_id = ' '.join(photo_id[:-1])\n",
    "                photo_number = int(photo_id[-1])\n",
    "                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
    "\n",
    "    photo_ids = pd.DataFrame(photo_ids)\n",
    "    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n",
    "\n",
    "    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n",
    "\n",
    "    #image preprocessing\n",
    "    all_photos =df['photo_path'].apply(skimage.io.imread)\\\n",
    "                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n",
    "                                .apply(lambda img: resize(img,[dimx,dimy]))\n",
    "\n",
    "    all_photos = np.stack(all_photos.values)#.astype('uint8')\n",
    "    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n",
    "\n",
    "    return all_photos, all_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем изображения и аттрибуты к ним\n",
    "data, attrs = fetch_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фиксируем `seed` для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config['seed'])\n",
    "random.seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(config['seed'])\n",
    "    device = torch.device('cuda:0')\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем выборку на `train` и `val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, train_attrs, val_attrs = train_test_split(data, attrs, test_size=0.2, random_state=config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем несколько изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3, sharey=True, sharex=True)\n",
    "\n",
    "for fig_x in ax.flatten():\n",
    "    idx = random.randrange(0, train_data.shape[0])\n",
    "    fig_x.imshow(train_data[idx])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем картинки к тензорам и создадим даталоадеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размерность изображений выглядит следующим образом (height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим, чтобы было: (channels, height, width) поэтому применяем permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.FloatTensor(train_data).permute(0,3,1,2)\n",
    "val_data = torch.FloatTensor(val_data).permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём даталоадеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=config['batch_size'])\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем класс автоэнкодера\n",
    "\n",
    "Полезные ссылки:\n",
    "* https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/\n",
    "* https://towardsdatascience.com/conv2d-to-finally-understand-what-happens-in-the-forward-pass-1bbaafb0b148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, dim_code, in_channels=3, out_channels=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.dim_code = dim_code\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels * 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.out_channels * 2, out_channels=self.out_channels * 4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            # 8 и 8, потому что у нас исходный размер картинок 64х64\n",
    "            # и мы сделали три свертки со страйдом = 2, поэтому\n",
    "            # 64 -> 32 -> 16 -> 8\n",
    "            nn.Linear(self.out_channels * 4 * 8 * 8, self.dim_code)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.dim_code, self.out_channels * 4 * 8 * 8),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(self.out_channels * 4, 8, 8)),\n",
    "            nn.ConvTranspose2d(in_channels=self.out_channels * 4, out_channels=self.out_channels * 2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=self.out_channels * 2, out_channels=self.out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=self.out_channels, out_channels=self.in_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # в качестве возвращаемых переменных:\n",
    "        # латентное представление картинки (latent_code)\n",
    "        # и полученная реконструкция изображения (reconstruction)\n",
    "        latent_code = self.encoder(x)        \n",
    "        reconstruction = self.decoder(latent_code)\n",
    "\n",
    "        return reconstruction, latent_code\n",
    "    \n",
    "    def get_latent(self, x):\n",
    "        # метод для получения только латентного представления\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def get_reconstruction(self, x):\n",
    "        # метод для получения реконструкции по входному латентному представлению\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализация начальных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "autoencoder = Autoencoder(dim_code=config['dim_code']).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(autoencoder.parameters(), lr=config['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, optimizer, criterion, train_loader, val_loader, scheduler):\n",
    "    target = next(iter(val_loader))\n",
    "    log = {\"epoch\": [], \"train_loss\": [],  \"val_loss\": []}\n",
    "    \n",
    "    for epoch in range(config['total_epochs']):\n",
    "        log['epoch'].append(epoch)\n",
    "        \n",
    "        avg_train_loss = 0\n",
    "        avg_val_loss = 0\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction, _ = model(data)\n",
    "            loss = criterion(reconstruction, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_train_loss += loss.item() / len(train_loader)\n",
    "        scheduler.step()\n",
    "        log['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # val\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                reconstruction, _ = model(data)\n",
    "                loss = criterion(reconstruction, data)\n",
    "                avg_val_loss += loss.item() / len(val_loader)\n",
    "        log['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        predict, _ = model(target.to(device))\n",
    "        # Visualize tools\n",
    "        clear_output(wait=True)\n",
    "        for k in range(6):\n",
    "            plt.subplot(2, 6, k+1)\n",
    "            plt.imshow(target[k].detach().cpu().permute(1,2,0))\n",
    "            plt.title('Target')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(2, 6, k+7)\n",
    "            plt.imshow(predict[k].detach().cpu().permute(1,2,0))\n",
    "            plt.title('Predict')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle('%d / %d - train_loss: %f, val_loss: %f' % \n",
    "                    (epoch+1, config['total_epochs'], log['train_loss'][-1], log['val_loss'][-1]))\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(log['epoch'], log['train_loss'], label='train')\n",
    "        plt.plot(log['epoch'], log['val_loss'], label='val')\n",
    "        plt.legend()\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title('Loss')\n",
    "        plt.show()\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = train(config,\n",
    "            autoencoder, optimizer, criterion,\n",
    "            train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем модельку и логи обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(autoencoder.state_dict(), f'{save_path}/autoencoder_model.pt')\n",
    "torch.save(log, f'{save_path}/autoencoder_log.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как автоэнкодер кодирует и восстанавливает картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = next(iter(val_loader))\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    predict, _ = autoencoder(target.to(device))\n",
    "    \n",
    "    for k in range(6):\n",
    "        plt.subplot(2, 6, k+1)\n",
    "        plt.imshow(target[k+6].detach().cpu().permute(1,2,0))\n",
    "        plt.title('Target')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(2, 6, k+7)\n",
    "        plt.imshow(predict[k+6].detach().cpu().permute(1,2,0))\n",
    "        plt.title('Predict')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сэмплинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_latents = torch.FloatTensor()\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for _data in train_loader:\n",
    "        _latent = autoencoder.get_latent(_data.to(device))\n",
    "        real_latents = torch.cat((real_latents, _latent.to('cpu')))\n",
    "        \n",
    "mu = torch.mean(real_latents, dim=0).to(device)\n",
    "sigma = torch.std(real_latents, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = autoencoder.get_latent(target.to(device))\n",
    "z = 0.5 * sigma * torch.randn(25, config['dim_code']).to(device) + mu\n",
    "# берем первый вектор\n",
    "fig, ax = plt.subplots(2)\n",
    "ax[0].hist(output[0].detach().cpu())\n",
    "ax[0].set_title('Real')\n",
    "ax[1].hist(z[0].detach().cpu())\n",
    "ax[1].set_title('Generated')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пропустим через модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction = autoencoder.get_reconstruction(z.to(device))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(reconstruction.shape[0]):\n",
    "  plt.subplot(5, 5, i + 1)\n",
    "  plt.imshow(reconstruction[i].detach().cpu().permute(1,2,0))\n",
    "  plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Вариационный автоэнкодер](https://proglib.io/p/variacionnye-avtoenkodery-vae-dlya-chaynikov-poshagovoe-rukovodstvo-2021-07-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализация конфига"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size'    : 32,\n",
    "    'total_epochs'  : 40,\n",
    "    'learning_rate' : 1e-4,\n",
    "    'save_path'     : './weights', # путь куда сохранять модельки\n",
    "    'seed'          : 42,\n",
    "    'features'      : 16 # длина вектора сэмплирования\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этот раз используем набор рукописных цифр [MNIST](https://blog.skillfactory.ru/glossary/mnist-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "val_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=config['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, features, in_channels=1, out_channels=32):\n",
    "        super().__init__()\n",
    "        # encoder должен кодировать картинку в 2 переменные -- mu и logsigma\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.features = features\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels * 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.out_channels * 2, out_channels=self.out_channels * 4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.out_channels * 4, out_channels=self.out_channels * 8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.out_channels * 8, out_channels=self.out_channels * 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels * 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Linear(self.out_channels * 16, self.features)\n",
    "        self.logsigma = nn.Linear(self.out_channels * 16, self.features)\n",
    "\n",
    "        # decoder\n",
    "        # отдельно делаем инпут для декодера, т.к. надо будет \n",
    "        # приводить размерность\n",
    "        self.decoder_input = nn.Linear(self.features, self.out_channels * 16)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=self.out_channels * 16, out_channels=self.out_channels * 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=self.out_channels * 8, out_channels=self.out_channels * 4, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels * 4),\n",
    "            nn.ReLU(),\n",
    "            # output_padding ставим 0, потому что у нас\n",
    "            # размер входа 28*28, иначе получим 32*32\n",
    "            nn.ConvTranspose2d(in_channels=self.out_channels * 4, out_channels=self.out_channels * 2, kernel_size=3, stride=2, padding=1, output_padding=0),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=self.out_channels * 2, out_channels=self.out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=self.out_channels, out_channels=self.in_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        mu = self.mu(x)\n",
    "        logsigma = self.logsigma(x)\n",
    "\n",
    "        return mu, logsigma\n",
    "\n",
    "    def gaussian_sampler(self, mu, logsigma):\n",
    "        if self.training:\n",
    "            # сэмплируем латентный вектор из нормального распределения с параметрами mu и sigma\n",
    "            return self.reparameterize(mu, logsigma)\n",
    "        else:\n",
    "            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu.\n",
    "            # на инференсе выход автоэнкодера должен быть детерминирован.\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.decoder_input(z)\n",
    "        z = z.view(-1, self.out_channels * 16, 1, 1)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logsigma = self.encode(x)\n",
    "        z = self.reparameterize(mu, logsigma)\n",
    "        reconstruction = self.decode(z)\n",
    "        return mu, logsigma, reconstruction\n",
    "    \n",
    "    def get_latent(self, x):\n",
    "        mu, logsigma = self.encode(x)\n",
    "        return self.reparameterize(mu, logsigma)\n",
    "    \n",
    "    def get_reconstruction(self, x):\n",
    "        return self.decode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определяем лосс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(mu, logsigma):\n",
    "    \"\"\"\n",
    "    часть функции потерь, которая отвечает за \"близость\" латентных представлений разных людей\n",
    "    \"\"\"\n",
    "    loss = -0.5 * torch.sum(1 + logsigma - mu.pow(2) - logsigma.exp())\n",
    "    return loss\n",
    "\n",
    "def log_likelihood(x, reconstruction):\n",
    "    \"\"\"\n",
    "    часть функции потерь, которая отвечает за качество реконструкции (как mse в обычном autoencoder)\n",
    "    \"\"\"\n",
    "    loss = nn.BCELoss(reduction='sum')\n",
    "    return loss(reconstruction, x)\n",
    "\n",
    "def loss_vae(x, mu, logsigma, reconstruction):\n",
    "    return KL_divergence(mu, logsigma) + log_likelihood(x, reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = loss_vae\n",
    "vae = VAE(features=config['features']).to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.AdamW(vae.parameters(), lr=config['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, epochs, train_loader, val_loader, scheduler):\n",
    "    target = next(iter(val_loader))\n",
    "    log = {\"epoch\": [], \"train_loss\": [],  \"val_loss\": []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        log['epoch'].append(epoch)\n",
    "        \n",
    "        avg_train_loss = 0\n",
    "        avg_val_loss = 0\n",
    "\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            data = data[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            mu, logsigma, reconstruction = model(data)\n",
    "            loss = criterion(data, mu, logsigma, reconstruction)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_train_loss += loss.item() / len(train_loader)\n",
    "        scheduler.step()\n",
    "        log['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data[0].to(device)\n",
    "                mu, logsigma, reconstruction = model(data)\n",
    "                loss = criterion(data, mu, logsigma, reconstruction)\n",
    "                avg_val_loss += loss.item() / len(val_loader)\n",
    "        log['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        _, _, predict = model(target[0].to(device))\n",
    "        # Visualize tools\n",
    "        clear_output(wait=True)\n",
    "        for k in range(6):\n",
    "            plt.subplot(2, 6, k+1)\n",
    "            plt.imshow(target[0][k].detach().cpu().permute(1,2,0))\n",
    "            plt.title('Target')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(2, 6, k+7)\n",
    "            plt.imshow(predict[k].detach().cpu().permute(1,2,0))\n",
    "            plt.title('Predict')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle('%d / %d - train_loss: %f, val_loss: %f' % \n",
    "                    (epoch+1, epochs, log['train_loss'][-1], log['val_loss'][-1]))\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(log['epoch'], log['train_loss'], label='train')\n",
    "        plt.plot(log['epoch'], log['val_loss'], label='val')\n",
    "        plt.legend()\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title('Loss')\n",
    "        plt.show()\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = train(vae, optimizer, criterion, config['total_epochs'],\n",
    "            train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем модельку и лог обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(autoencoder.state_dict(), f'{save_path}/vae_model.pt')\n",
    "torch.save(log, f'{save_path}/vae_log.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как вае кодирует и восстанавливает картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = next(iter(val_loader))\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    _, _, predict = vae(target[0].to(device))\n",
    "    \n",
    "    for k in range(6):\n",
    "        plt.subplot(2, 6, k+1)\n",
    "        plt.imshow(target[0][k+6].detach().cpu().permute(1,2,0))\n",
    "        plt.title('Target')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(2, 6, k+7)\n",
    "        plt.imshow(predict[k+6].detach().cpu().permute(1,2,0))\n",
    "        plt.title('Predict')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сэмплинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([np.random.normal(0, 1, config['features']) for _ in range(25)])\n",
    "with torch.no_grad():\n",
    "    output = vae.get_reconstruction(torch.FloatTensor(z).to(device))\n",
    "fig, ax = plt.subplots(5, 5, figsize=(6, 6))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        axes = ax[i, j]\n",
    "        axes.imshow(output[5*i+j].cpu().squeeze(0))\n",
    "        axes.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
