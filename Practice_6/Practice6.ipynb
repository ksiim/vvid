{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое занятие №6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu langchain langchain-community beautifulsoup4 transformers sentence-transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.utils.html import (PREFIXES_TO_IGNORE_REGEX,\n",
    "                                  SUFFIXES_TO_IGNORE_REGEX)\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import Pooling, Transformer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import unicodedata\n",
    "import torch\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'llm'       : 'IlyaGusev/saiga_llama3_8b',\n",
    "    'encoder'   : 'cointegrated/rubert-tiny2',\n",
    "    'url'       : 'https://tinyurl.com/34kex5my'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    def __init__(self, config):\n",
    "        self.__config = config\n",
    "        \n",
    "        self.__tokenizer, self.__llm = self.__get_llm(self.__config['llm'])\n",
    "        \n",
    "        self.__encoder = self.__get_encoder(self.__config['encoder'])\n",
    "        \n",
    "        data = self.__get_data(self.__config['url'])\n",
    "        chunks = self.__get_chunks(data)\n",
    "        \n",
    "        self.__retriever = self.__get_retriever(chunks)\n",
    "        \n",
    "    def __get_llm(self, model_id):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        llm = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        \n",
    "        return tokenizer, llm\n",
    "    \n",
    "    def __get_data(self, url):\n",
    "        loader = UnstructuredURLLoader(urls=[url])\n",
    "        documents = loader.load()\n",
    "        \n",
    "        return documents\n",
    "\n",
    "    def __get_chunks(self, documents, chunk_size=3000, chunk_overlap=500):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, \n",
    "                                                       chunk_overlap=chunk_overlap, \n",
    "                                                       is_separator_regex = False,\n",
    "                                                       add_start_index = False)\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def __get_encoder(self, encoder_name):\n",
    "        return SentenceTransformerEmbeddings(model_name=encoder_name)\n",
    "    \n",
    "    def __get_retriever(self, chunks):\n",
    "        index = FAISS.from_documents(chunks, self.__encoder)\n",
    "        retriever = index.as_retriever()\n",
    "        \n",
    "        return retriever\n",
    "    \n",
    "    def __get_context(self, query):\n",
    "        contexts = [unicodedata.normalize('NFKD', docs.page_content) for docs in self.__retriever.get_relevant_documents(query)]\n",
    "        context = '.'.join(contexts)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def __get_response(self, query, context, max_new_tokens=300, temperature=0.6, top_p=0.18, top_k=100):\n",
    "        user_prompt = '''\n",
    "        Используй фрагменты полученного контекста, чтобы ответить на вопрос. \n",
    "        Если ты не знаешь ответа, то скажи, что не знаешь, не придумывай ответ. \n",
    "        Используй максимум три предложения и отвечай кратко.\\n\n",
    "        Контекст:\\n\n",
    "        {context}\\n\n",
    "        Вопрос:\\n\n",
    "        {query}'''.format(context=context, query=query)\n",
    "        \n",
    "        SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "        RESPONSE_TEMPLATE = \"Ответ: \"\n",
    "        \n",
    "        prompt = f'''\n",
    "                    <|begin_of_text|><|start_header_id|>system<|end_header_id|> \\\n",
    "                    {SYSTEM_PROMPT} \\\n",
    "                    <|eot_id|><|start_header_id|>user<|end_header_id|> \\\n",
    "                    {user_prompt} \\\n",
    "                    <|eot_id|><|start_header_id|>assistant<|end_header_id|> \\\n",
    "                    {RESPONSE_TEMPLATE}\n",
    "                '''\n",
    "        \n",
    "        def generate(model, tokenizer, prompt):\n",
    "            data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "            data = {k: v.to(model.device) for k, v in data.items()}\n",
    "            output_ids = model.generate(\n",
    "                **data,\n",
    "                bos_token_id=128000,\n",
    "                eos_token_id=128001,\n",
    "                pad_token_id=128001,\n",
    "                do_sample=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                no_repeat_ngram_size=15,\n",
    "                repetition_penalty=1.0,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p \n",
    "            )[0]\n",
    "            output_ids = output_ids[len(data[\"input_ids\"][0]) :]\n",
    "            output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "            \n",
    "            return output.strip()\n",
    "        \n",
    "        response = generate(self.__llm, self.__tokenizer, prompt)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_answer(self, query):\n",
    "        context = self.__get_context(query)\n",
    "        response = self.__get_response(query, context)\n",
    "        response = response.split('assistant')[0]\n",
    "        \n",
    "        answer = \"\"\"\n",
    "        Вопрос: {query}\\n\n",
    "        =======================\\n\n",
    "        Ответ: {response}\n",
    "        \"\"\".format(query=query, response=response)\n",
    "        \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'В каком году вышел фильм?'\n",
    "answer = rag.get_answer(query)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
